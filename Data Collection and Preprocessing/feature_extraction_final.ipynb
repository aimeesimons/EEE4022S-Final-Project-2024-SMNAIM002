{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import joblib\n",
    "import random\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wavelet(signal, i ,j,scales,data_cwt, start,\n",
    "                 waveletname = 'morl'):\n",
    "    start = 60 + start\n",
    "    [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname)\n",
    "    coefficients = coefficients[:127,start:start+127]\n",
    "    data_cwt[j,:,:,i] = np.abs(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_levels = [0, 20, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_detect.pickle\", \"rb\") as f:\n",
    "    X_detect = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1 = 40000\n",
    "X_detect_input = np.ndarray(shape=(size1, 127, 127, 6), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for k, items in enumerate(X_detect):\n",
    "    i = 0\n",
    "    snr = np.random.choice(snr_levels)\n",
    "    start = int(random.random()*0.1*2000)\n",
    "    for (columnName,columnData) in items.items(): \n",
    "        input_signal = columnData[:1000].values\n",
    "        if max(input_signal)<1:\n",
    "            input_signal = input_signal*666\n",
    "        if snr == 0:\n",
    "            filtered_signal = input_signal\n",
    "        else:\n",
    "            signal_original = add_all_noises(input_signal/5,noise_factors,snr)   \n",
    "            if \"I\" in columnName:\n",
    "                signal_original = signal_original/666\n",
    "            b,a = scipy.signal.cheby1(9, 1, 100, fs=2000, btype='lowpass')\n",
    "            filtered_signal = scipy.signal.filtfilt(b, a, signal_original)\n",
    "        signal_normalised = preprocessing.normalize([filtered_signal])\n",
    "        compute_wavelet(signal_normalised.reshape(-1),i,j,scales=np.arange(1,128),data_cwt=X_detect_input, start=start)\n",
    "        i += 1\n",
    "    j += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_detect_input, \"X_detect_input_noisy_varied1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_classify.pickle\", \"rb\") as f:\n",
    "    X_classify = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size2 = 22000\n",
    "X_classify_input = np.ndarray(shape=(size2, 127, 127, 6), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for k, items in enumerate(X_classify):\n",
    "    i = 0\n",
    "    snr = np.random.choice(snr_levels)\n",
    "    start = int(random.random()*0.1*2000)\n",
    "    for (columnName,columnData) in items.items(): \n",
    "        input_signal = columnData[:1000].values\n",
    "        if max(input_signal)<1:\n",
    "            input_signal = input_signal*666\n",
    "        if snr == 0:\n",
    "            filtered_signal = input_signal\n",
    "        else:\n",
    "            signal_original = add_all_noises(input_signal/5,noise_factors,snr)   \n",
    "            if \"I\" in columnName:\n",
    "                signal_original = signal_original/666\n",
    "            b,a = scipy.signal.cheby1(9, 1, 100, fs=2000, btype='lowpass')\n",
    "            filtered_signal = scipy.signal.filtfilt(b, a, signal_original)\n",
    "        signal_normalised = preprocessing.normalize([filtered_signal])\n",
    "        compute_wavelet(signal_normalised.reshape(-1),i,j,scales=np.arange(1,128),data_cwt=X_classify_input, start=start)\n",
    "        i += 1\n",
    "    j += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_classify_input, \"X_classify_input_noisy_varied1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wavelet(signal, i ,j,scales, data_cwt, \n",
    "                 waveletname = 'morl'):\n",
    "    \n",
    "    [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname)\n",
    "    coefficients = coefficients[:40,:40]\n",
    "    data_cwt[j,:,:,i] = np.abs(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_levels = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size2 = 1010\n",
    "X_locate_input = np.ndarray(shape=(size2, 80, 80, 6), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.read_csv(\"C:\\\\Users\\\\Aimee Simons\\\\Desktop\\\\2024\\\\Lectures\\\\Semester 2\\\\Final Thesis\\\\Model\\\\Fault Localisation\\\\Tester Model\\\\Line Lengths.csv\")\n",
    "lines = lengths['Line'].values\n",
    "for line in lines:\n",
    "    with open(f\"Location Data2\\\\{line}\\\\X_locate_{line}_training.pickle\", 'rb') as f:\n",
    "        X_locate = pickle.load(f)\n",
    "    for snr in snr_levels:\n",
    "        j = 0\n",
    "        for items in X_locate:\n",
    "            i = 0\n",
    "            # snr = np.random.choice(snr_levels)\n",
    "            for (columnName,columnData) in items.items():\n",
    "                input_signal = columnData.values\n",
    "                factor = 400/max(input_signal)\n",
    "                input_signal = input_signal*factor\n",
    "                if snr == 0 or snr== '_noNoise':\n",
    "                    filtered_signal = input_signal/factor\n",
    "                    snr='_noNoise'\n",
    "                else:\n",
    "                    signal_original = add_all_noises(input_signal/5,noise_factors,snr)\n",
    "                    signal_original = signal_original/factor \n",
    "                    b,a = scipy.signal.cheby1(9, 1, 100, fs=2000, btype='lowpass')\n",
    "                    filtered_signal = scipy.signal.filtfilt(b, a, signal_original)\n",
    "                signal_normalised = preprocessing.normalize([filtered_signal])\n",
    "                compute_wavelet(signal_normalised.reshape(-1),i,j,scales=np.arange(1,81),data_cwt=X_locate_input)\n",
    "                i += 1\n",
    "            j += 1 \n",
    "        with open(f\"LocationData\\\\{line}\\\\X_locate_input_train{snr}.pickle\", 'wb') as f:\n",
    "            pickle.dump(X_locate_input, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderOfTypes =['ABC','ABG','AB','ACG','AC','AG','BCG','BC','BG','CG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size2 = 15\n",
    "X_locate_input = np.ndarray(shape=(size2, 40, 40, 6), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.read_csv(\"C:\\\\Users\\\\Aimee Simons\\\\Desktop\\\\2024\\\\Lectures\\\\Semester 2\\\\Final Thesis\\\\Model\\\\Fault Localisation\\\\Tester Model\\\\Line Lengths.csv\")\n",
    "lines = lengths['Line'].values\n",
    "for line in lines:\n",
    "    with open(f\"Location Data2\\\\{line}\\\\X_locate_{line}_testing.pickle\", 'rb') as f:\n",
    "        X_locate = pickle.load(f)\n",
    "    for k, fault in enumerate(orderOfTypes):\n",
    "        j = 0\n",
    "        for items in X_locate[k]:\n",
    "            i = 0\n",
    "            snr = np.random.choice(snr_levels)\n",
    "            for (columnName,columnData) in items.items():\n",
    "                input_signal = columnData.values\n",
    "                factor = 400/max(input_signal)\n",
    "                input_signal = input_signal*factor\n",
    "                if snr == 0 or snr== '_noNoise':\n",
    "                    filtered_signal = input_signal/factor\n",
    "                    snr='_noNoise'\n",
    "                else:\n",
    "                    signal_original = add_all_noises(input_signal/5,noise_factors,snr)\n",
    "                    signal_original = signal_original/factor \n",
    "                    b,a = scipy.signal.cheby1(9, 1, 100, fs=2000, btype='lowpass')\n",
    "                    filtered_signal = scipy.signal.filtfilt(b, a, signal_original)\n",
    "                signal_normalised = preprocessing.normalize([filtered_signal])\n",
    "                compute_wavelet(signal_normalised.reshape(-1),i,j,scales=np.arange(1,81),data_cwt=X_locate_input)\n",
    "                i += 1\n",
    "            j += 1 \n",
    "        with open(f\"LocationData\\\\{line}\\\\X_locate_input_test_{fault}.pickle\", 'wb') as f:\n",
    "            pickle.dump(X_locate_input, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
